
---
title: "PADP8120 Midterm Fall 2015"
author: "Thomas K. Valentine"
date: "October 14, 2015"
output:
  html_document:
    highlight: tango 
    theme: united
    
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
---
***
# Data Source

We will analyze data from a New York City school voucher experiment (obtained by Dr. Tyler Scott and a colleague) from Mathematica.

```{r eval=TRUE}
nyc = read.csv('input/NYC_SchoolVoucher_Experiment.csv',row.names=1)
```

***

# Variable Key
| Variable | Description |
|:---|:---|
| `Student_ID` | numerical code for each student |
| `Family_ID` | numerical code for each family |
| `Date.Of.Birth` | student birthday |
| `Female` | binary indicator for female students |
| `Grade`   |  student grade level when starting experiment |
| `Treatment`    |  binary indicator for whether student received voucher |
| `Latino`  |   binary indicator for Latino students |
| `Black` |   binary indicator for black students |
| `Eldest` |   binary indicator for eldest children |
| `y0_read_percentile`   |  pre-test reading score percentile |
| `y0_math_percentile`    |  pre-test math score percentile |
| `y1_read_percentile`   |  reading score percentile after 1y |
| `y1_math_percentile`    |  math score percentile after 1y |
| `y2_read_percentile`   |  reading score percentile after 2y |
| `y2_math_percentile`    |  math score percentile after 2y |
| `y3_read_percentile`   |  reading score percentile after 3y |
| `y3_math_percentile`    |  math score percentile after 3y |

***
#Problems

###1. (6 points) Compute the average reading and math percentiles for each year across all students. 

Using In-Line R Code, we can demonstrate the following values from our "nyc" data set:

####Year 0
1. Average Reading % (across all students) is  <span style="color:green">_`r round(mean(nyc$y0_read_percentile,na.rm=T),2)`_</span>
2. Average Math % (across all students) is  <span style="color:green">_`r round(mean(nyc$y0_math_percentile,na.rm=T),2)`_</span>

####Year 1
1. Average Reading % (across all students) is  <span style="color:green">_`r round(mean(nyc$y1_read_percentile,na.rm=T),2)`_</span>
2. Average Math % (across all students): is  <span style="color:green">_`r round(mean(nyc$y1_math_percentile,na.rm=T),2)`0_</span>

####Year 2
1. Average Reading % (across all students) is  <span style="color:green">_`r round(mean(nyc$y2_read_percentile,na.rm=T),2)`_</span>
2. Average Math % (across all students) is  <span style="color:green">_`r round(mean(nyc$y2_math_percentile,na.rm=T),2)`_</span>

####Year 3
1. Average Reading % (across all students) is  <span style="color:green">_`r round(mean(nyc$y3_read_percentile,na.rm=T),2)`_</span>
2. Average Math % (across all students) is  <span style="color:green">_`r round(mean(nyc$y3_math_percentile,na.rm=T),2)`0_</span>

####Analysis:
*Reading: Increased significantly from year 0-1, then remained mostly unchanged from year 1-2, then increased again from year 2 to 3

*Math: Followed a very similar trend to math. Large increase from 0-1, stable from 1-2, increase from 2-3.

***
###2. (4 points) Clean the original dataset by removing all students who do not have observe pre-test scores and year 1 scores for reading and for math.

```{r}
#We will create a new "clean" data set. To do so, we will create a subset of the original data set "nyc", where any observations contain blank ("NA") reading or math scores in Year Zero or Year 1.
nyc_clean <- subset(nyc, y0_read_percentile != "NA" & y0_math_percentile != "NA" & y1_read_percentile != "NA" & y0_read_percentile != "NA")

#Let's compare the dimensions of our original to our new data set
dim(nyc)
dim(nyc_clean)
```

Our new data set, nyc_clean, kept the 17 variables from the original data set, but lost 1211 observations during this process. We were able to confirm visually (from inside R Studio) that the cleaning process successfully removed observations with missing scores in the correct columns/variables.

***
###3. (8 points) Make a figure (either a density plot or a histogram format) comparing the distributions of percentile reading scores for students who did and did not receive a voucher. Do your best to make the figure "publishable" by giving it clear axis titles, legend labels, and other aesthetic improvements. 

```{r}
library(dplyr)
library(ggplot2)

#First, we create a new variable, readingscores, that is the mean reading score for each student
nyc_clean$Reading <- (nyc_clean$y0_read_percentile + nyc_clean$y1_read_percentile + nyc_clean$y2_read_percentile + nyc_clean$y3_read_percentile)/4

#Second, we create a data set that contains only reading scores and treatment value
nycreading <- select(nyc_clean, Treatment, Reading)

#Next, we reorder the columns so we have our continuous variable (reading scores) first, then our binary treatment effect (0,1) second.
nycreading <- nycreading[c("Reading", "Treatment")]

#We omit observations with missing values, which drops 462 observations
nycreading <- na.omit(nycreading)

library(plyr)

#We rename the columns to suit our goals for the figure
nycreading <- rename(nycreading, c("Treatment"="Voucher", "Reading"="ReadingScores"))

#We convert our binary 0 and 1 values to 'Yes' and 'No' so that we can view the treatment groups as factors
nycreading[nycreading==0] <- 'No'
nycreading[nycreading==1] <- 'Yes'

#One item under Reading Scores is incorrectly coded in this process. We fix it here:
nycreading$ReadingScores[nycreading$ReadingScores=='Yes'] <- 1.00

#We recode Voucher as a Factor
nycreading$Voucher <- as.factor(nycreading$Voucher)
nycreading$ReadingScores <- as.numeric(nycreading$ReadingScores)

#Now, we can create our density plot:
p <- ggplot(nycreading, aes(x=ReadingScores)) + geom_density(aes(group=Voucher, colour=Voucher, fill=Voucher), alpha=0.3)

p + labs(title = "Voucher Effects on Reading Scores in NYC Schools") + xlab("Mean Student Reading Scores - Year 0-3") + ylab("Density of Scores")

```

***
###4. (4 points) In no more than 2-3 sentences, describe what your visualization in 1.C indicates about these data. Do the treatment and control groups appear to be similar (in terms of reading pre-test scores)? Why or why not? 

These data appear to be related. They are similarly distributed (with a strong left skew that peaks near the 12.5% score mark), although the students that received the vouchers skew slightly farther left, demonstrating concentrations of slightly lower scores at all but around the 60-65% grade mark. As a result, both the treatment (VOucher=Yes) and Control (Voucher=No) appear to be remarkably similar in reading scores, and we can expect that pre-test scores also fall in this range.


***
###5. (4 points) Typically, policy experiments such as the NYC school voucher lottery experiment are indended to produce statistical evidence that can then be generalized to broader policy applications (i.e., how might vouchers influence educational outcomes for other students who weren't part of the original experiment). Based on the reading pre-test scores that you visualized in C, describe any concerns you might have about our ability to generalize the results of this experiment to ALL elementary school students in the United States in no more than 2-3 sentences. 

<span style="color:green">**First let me say that I've been reminded of an important lesson: always read all the questions before beginning. Had I done so, I would have seen the repeated references to "pre-test" scores (instead of assuming it meant all reading scores across years, since the question didn't specify) and I could have saved myself close to 6 hours of headache. Lesson learned**</span>

My first concern is that there does not seem to be a significant difference in reading score density between the control and treatment group, with both lacking a normal distribution (which is a big flag). My second concern is the fact that (due to missed responses that we have not yet determined were missing at random) we went from 2666 observations to a visualized 993 observations. Pre-testing can lead to problems with external validity, and could potentially explain the similarity of results before and after the voucher experiment.

####Bonus: To satisfy my curiousity, I've decided to quickly take a look at the ggplot for Pre-Test Reading Scores. After looking, I'm satisfied that I would have come to largely the same conclusions.
```{r echo=FALSE}
p1 <- ggplot(nyc_clean, aes(x=y0_read_percentile)) + geom_density(aes(group=Treatment, colour=Treatment, fill=Treatment), alpha=0.3)

p1 + labs(title = "Pre-Test Reading Scores in NYC Schools") + xlab("Mean Student Reading Scores") + ylab("Density of Scores")
```


```{r}
summary(cars)
```

***
###6. (4 points) What is the probability that a randomly selected student in the dataset received a voucher and scored above the 50th percentile on the math pre-test?

```{r}
#First, we will use the Shapiro-Wilks test to formally check if the sample (students that received a score on the math pre-test) is normally distributed.
shapiro <- shapiro.test(nyc_clean$y0_math_percentile)
```
Our resulting p-value is < 2.2e-16, or less than 0.00000000000000022. We can use in-line r code to find out that the actual p-value is `r shapiro$p.value`. With a p-value far less than .05, we have an infinitesimal chance of a normal distribution. We can reject the null hypothesis of normality.

```{r}
#Next, we can find out more about the value of the math pre-test sample, including the value of the Mean or 50% percentile mark:
summary(nyc_clean$y0_math_percentile)
quantile(nyc_clean$y0_read_percentile, c(.25, .50,  .75, .90, .99))
```

So, here's what we know: The 50th percentile is 16. We have 1455 observations.This are not disjoint/mutally exclusive outcomes. We do not have a normal distribution. We can not confidently say the outcomes are independent of one another. If we say that receiving a voucher is Event A and that scoring about the 50% percentile is Event B, we know that: $$P(A and B) = P(A | B) P(B)$$ Another way of saying this is: $$P(Voucher and Above50) = P(Voucher | Above50) P(Above50)$$ Since we don't have a normal distribution, we are prohibited from using some of the useful pnorm type commands. Instead, we will use a frequency table.   

```{r}
#In the following frequency table, Treatment (Receiving a Voucher) will be the rows, scoring above 16.42 (scoring above the mean) will be the columns.
mytable <- table(nyc_clean$Treatment==1, nyc_clean$y0_math_percentile>16)

#We can conduct a chi-square test on this table to determine if these are independent outcomes.
chisq.test(mytable)
```

As the p value of the Pearson's chi-square test is signficantly above .05, we do not reject the null hypothesis that receiving a voucher is independent of scoring above 50. This allows us to now use the multiplicaton rule for independent processes. So:
$$P(AandB)=P(A)xP(B)$$.

```{r}
#Now, let's see the results of the table itself
table(nyc_clean$Treatment==1, nyc_clean$y0_math_percentile>16)
```

Using this table, we see that 282 out of the 1455 students are above the 50% percentile AND have received a voucher. This allows us to determine that the probability a randomly selected student in the dataset received a voucher and scored above the 50th percentile on the math pre-test is 282/1455, or 19%.

Let's confirm this with the P(AandB)=P(A)xP(B) formula. From the table, we can see that 233+282 or 515 students scored above the 50th percentile. This puts the odds of Event A as 515/1455 or 35%. From the table, we can see that Event B would similarly be 781/1455 or 54%. If we use the multiplication rules, we get 35% x 54%, which results in 19%. This confirms our earlier calculation.

<span style="color:green">The probability that a randomly selected student in the dataset received a voucher and scored above the 50th percentile on the math pre-test is 19%</span>

***
###7. (4 points) What is the probability that a randomly selected student who received a voucher group scored above the 50th percentile on the math pre-test?

<span style="color:green">It is not immediately clear how this question is different than Question 6. Both seem to ask what is the probability that a randomly selected student in the dataset received a voucher and scored above the 50th percentile. I'm not sure what the significance of "group scored" or how one could receive a "voucher group" so I'm not sure how to proceed.My best guess is that one of these questions was intended to ask the odds of both events occuring ("AND") as opposed to the odds that one or the other event occurs ("OR"), so I have solved this question for the or. </span>

As opposed to the previous question, where we used the P(AandB)=P(A)xP(B) formula, we instead use the following formula: $$P(A or B) = P(A) + P(B) - P(A and B)$$ We can use the values we calculated in Question 6 to fill in this probabilities. Therefore we know that:
```{r}
#P(A or B) = P(A) + P(B) - P(A and B)
515/1455 + 781/1455 - 282/1455
```

<span style="color:green">The probability that a randomly selected student received a voucher OR scored above the 50th percentile on the math pre-test is 69.6%</span>

***
###8. (4 points) We are interested in studying whether the change in math scores after one year is different for students who receive a voucher versus those who do not. Write the null and alternative hypotheses in words and then using symbols.

```{r}
#Before we begin, we need to know the average change in math scores after one year (from year 0 to year 1) for the students that did not receive a voucher:
mathchg <- mean(nyc_clean$y1_math_percentile[nyc_clean$Treatment==0])-mean(nyc_clean$y0_math_percentile[nyc_clean$Treatment==0])
```

Now that we know that the average change in math scores after one year is `r round(mathchg, 2)`, we can construct our hypotheses:

**Null Hypothesis:** Students who receive a voucher experience a change in math scores after one year that is equal to `r round(mathchg, 2)`.

**Alternative Hypothesis:** Students who receive a voucher experience a change in math scores after one year that is not equal to `r round(mathchg, 2)`.

$$H_0: \mu == `r round(mathchg, 2)`$$
$$H_A: \mu != `r round(mathchg, 2)`$$

***
###9. (6 points) Evaluate the hypothesis from 8 at a significance level of α=0.05 and state your conclusion in 1-2 sentences. 

```{r}
#First, we will create a new variable that consists of the differences in math scores after one year for each student:
nyc_clean$MathAfter <- (nyc_clean$y1_math_percentile - nyc_clean$y0_math_percentile)
```

We now have a variable that we can test. we are interested in positive and negative changes, so we are interested in two-tailed tests.

```{r}
t.test(x=nyc_clean$MathAfter[nyc_clean$Treatment==1],mu=mathchg,alternative='two.sided')
```

Having tested the target sample (the 780 students that received a voucher) and compared it to the true mu of the entire population (all students), our t-test reveals that we have a p-value of 0.3725.  Due to the high value of the P, which is significantly higher than the .05 alpha, we fail to reject the null hypothesis that students who receive a voucher experience a change in math scores after one year that is equal to `r round(mathchg, 2)` or (to put it another way) we cannot support the claim that those that receive vouchers experienced a difference in math scores after a year.

Note: As means of double checking the quality of our calculation, we can confirm that the mean is within the 95 percent confidence interval.

***
###10. (2 points) Would your conclusion change at $\alpha = 0.10$? Why or why not?

<span style="color:green">No our conclusion would not change.</span>  If our alpha was increased to .10, reflecting a 90% confidence interval, we would still be unable to reject the null. Our p-value of 0.3725 is still significantly higher than the .10 alpha.

***
###11. (6 points) There are four assumptions that hold for the test above to be valid: independence within groups, independence between groups, sample sizes both above 30, and symmetric distributions. Are you satisfied that each of these are upheld? Is there anything in particular that you might be concerned about? 

```{r}
#First, I would like to collect some basic information on each group
library(pastecs)
stat.desc(nyc_clean$MathAfter[nyc_clean$Treatment==1])
stat.desc(nyc_clean$MathAfter[nyc_clean$Treatment==0])
#Note: The # of Values is 1 more than accurate; it does not take into account the header
```

<span style="color:green">Assumption One: Independence Within Groups: </span>
Independence is a good assumption. It seems unlikely the performance of one student within the group would influence the performance of another student within the same group.

<span style="color:green">Assumption Two: Independence Between Groups: </span>
Independence is again a good assumption. It seems unlikely that the performance of a student in a group could influence the performance of a student in another group.

<span style="color:green">Assumption Three: Sample Sizes Both Above 30: </span> 
 As we confirmed in the stat.desc commands, the Voucher group is a sample of 780 people, while the Non-Voucher group is a saple of 673 people. Both groups meet the standard of this requirement. We could also have accomplish this with the table function, if desired. No matter what, we have confirmed n>30.

<span style="color:green">Assumption Four: Symmetric Distributions </span>
```{r}
#Let's plot the two groups to take a look at their symmetry:
library(ggplot2)
ggplot(nyc_clean,aes(x=nyc_clean$MathAfter,group=nyc_clean$Treatment,col=nyc_clean$Treatment)) + geom_density() + theme_bw()
```

As we see, symmetry is supported for both groups. We are satisfied that each of these are upheld. It feels callous to say there is nothing we are concerned about (especially on a test where I'm certain we are meant to notice something), but all I can mention is that the symmetry may not be quite perfect. We can see some changes on the right side of the distribution, possibly indicating some important phenomena. Mostly, however, I think we should feel satisfied.

***
###12. (4 points) Next, you are asked to evaluate whether students in the treatment group differ in terms of their reading ability from the pre-test to the year 1 post-test. Write out the null and alternative hypotheses in words and then in symbols. 

```{r}
#Before we begin, we need to know the average change in reading scores after one year (from year 0 to year 1) for the students that did not receive a voucher:
readchg <- mean(nyc_clean$y1_read_percentile[nyc_clean$Treatment==0])-mean(nyc_clean$y0_read_percentile[nyc_clean$Treatment==0])
```

Now that we know that the average change in read scores after one year is `r round(readchg, 2)`, we can construct our hypotheses:

**Null Hypothesis:** Students who receive a voucher experience a change in reading scores after one year that is equal to `r round(readchg, 2)`.

**Alternative Hypothesis:** Students who receive a voucher experience a change in read scores after one year that is not equal to `r round(readchg, 2)`.

$$H_0: \mu == `r round(readchg, 2)`$$
$$H_A: \mu != `r round(readchg, 2)`$$


***
###13. (6 points) Conduct a t-test to evaluate this hypothesis at $\alpha = 0.01$ significance level. Interpret your results in 1-2 complete sentences.  
 
```{r}
#First, we will create a new variable that consists of the differences in math scores after one year for each student:
nyc_clean$ReadAfter <- (nyc_clean$y1_read_percentile - nyc_clean$y0_read_percentile)
```

We now have a variable that we can test. we are interested in positive and negative changes, so we are interested in two-tailed tests.

```{r}
t.test(x=nyc_clean$ReadAfter[nyc_clean$Treatment==1],mu=readchg,conf.level = 0.99,alternative='two.sided')
```

Having tested the target sample (the 780 students that received a voucher) and compared it to the true mu of the entire population (all students), our t-test reveals that we have a p-value of 0.03434.  Due to the  value of the P, which is slightly higher than the .01 alpha, we fail to reject the null hypothesis that students who receive a voucher experience a change in reading scores after one year that is equal to `r round(readchg, 2)` or (to put it another way) we cannot support the claim that those that receive vouchers experienced a difference in reading scores after a year.

Note: As means of double checking the quality of our calculation, we can confirm that the mean is within the 99 percent confidence interval.

*** 
###14. (4 points) Explain how you could evaluate the hypothesis above using a confidence interval instead of a p-value.

```{r}

```


***
###15. (2 points) Interpret the confidence interval you estimated for question N in a sentence.
<span style="color:green">NOTE: Again, I'm not quite sure about the phrasing of this question. It refers to a question N? I'm going to operate on the assumption that it is referring to Question 14.</span>

```{r}

```

***
###16. (8 points) Perform a test of whether the average math pre-test score differs by grade level at $\alpha = 0.10$ significance level. Write out your hypothesis in symbols, perform the test, and then report your findings in a complete sentence.

```{r}

```

***
###17. (8 points) The NYC School District considers students who perform at or above the 50th percentile to be "passing". At a 99% significance level, is the proportion of all students in the sample (voucher and no-voucher) that are considered to be passing in math significantly different between the pre-test and year 1 post-test?

```{r}

```

***
###18. (6 points) Make a boxplot that compares the distribution of math scores between the pre-test and year 1 post-test for all students. Do your best to make the figure "publishable" by giving it clear axis titles, legend labels, and other aesthetic improvements. In no more than 2-3 sentences, describe how your figure supports (or does not support) your finding from question 17.

```{r}

```

***
###19. (4 points) Consider the correlation between math percentile and reading percentile for year 1 test scores. Compute Pearson’s `r` between these two variables. Is the correlation higher or lower than you would have expected? 

```{r}

```

***
###20. (6 points) You might argue that percentiles are actually rank-order data and not ratio-level data. For rank-order data, a more appropriate correlation measure is Spearman’s $\rho$, which is a rank correlation. Compute $\rho$ and contrast the results from $r$ (for question S above). Why do the results differ?

```{r}

```

***
#### The command below is for debugging and was adopted from our homework keys

```{r echo=FALSE}
sessionInfo()
```